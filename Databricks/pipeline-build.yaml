# Specify the trigger event to start the build pipeline.
# In this case, new code merged into the release branch initiates a new build.
# Only include the folder paths that are relevant for your pipeline.

trigger:
  branches:
    include:
      - master
  paths:
    include:
    - Databricks 

stages:
- stage: Build
  displayName: 'Build'
  jobs:
  - job: Build
    displayName: 'Build'
    pool:
      vmImage: ubuntu-20.04
    steps:
  # Install Python. The version of Python must match the version on the
  # Azure Databricks cluster. This pipeline assumes that you are using
  # Databricks Runtime 10.4 LTS on the cluster.
      
      - task: UsePythonVersion@0
        displayName: 'Use Python 3.8'
        inputs:
          versionSpec: 3.8

      # Install required Python modules and their dependencies. These
      # include pytest, which is needed to run unit tests on a cluster,
      # and setuptools, which is needed to create a Python wheel. Also
      # install the version of Databricks Connect that is compatible
      # with Databricks Runtime 10.4 LTS on the cluster.
      - script: |
          pip install pytest requests setuptools wheel
          pip install -U databricks-connect==10.4.*
        displayName: 'Load Python dependencies'

      # Use environment variables to pass Azure Databricks workspace and cluster
      # information to the Databricks Connect configuration function.
      - script: |
          echo "y
          $(DATABRICKS_ADDRESS)
          $(DATABRICKS_API_TOKEN)
          $(DATABRICKS_CLUSTER_ID)
          $(DATABRICKS_ORG_ID)
          $(DATABRICKS_PORT)" | databricks-connect configure
        displayName: 'Configure Databricks Connect'

      # Download the files from the designated branch in the Git remote repository
      # onto the build agent.
      - checkout: self
        persistCredentials: true
        clean: true

      # For library code developed outside of an Azure Databricks notebook, the
      # process is like traditional software development practices. You write a
      # unit test using a testing framework, such as the Python pytest module, and
      # you use JUnit-formatted XML files to store the test results.
      - script: |
          python -m pytest --junit-xml=$(Build.Repository.LocalPath)/logs/TEST-LOCAL.xml $(Build.Repository.LocalPath)/libraries/python/dbxdemo/test*.py || true
        displayName: 'Run Python unit tests for library code'

      # Publishes the test results to Azure DevOps. This lets you visualize
      # reports and dashboards related to the status of the build process.
      - task: PublishTestResults@2
        inputs:
          testResultsFiles: '**/TEST-*.xml'
          failTaskOnFailedTests: true
          publishRunAttachments: true

      # Package the example Python code into a Python wheel.
      - script: |
          cd $(Build.Repository.LocalPath)/Databricks/libraries/python/dbxdemo
          python3 setup.py sdist bdist_wheel
          ls dist/
        displayName: 'Build Python Wheel for Libs'

      # Generate the deployment artifacts. To do this, the build agent gathers
      # all the new or updated code to be deployed to the Azure Databricks
      # environment, including the sample Python notebook, the Python wheel
      # library that was generated by the build process, related release settings
      # files, and the result summary of the tests for archiving purposes.
      # Use git diff to flag files that were added in the most recent Git merge.
      # Then add the Python wheel file that you just created along with utility
      # scripts used by the release pipeline.
      # The implementation in your pipeline will likely be different.
      # The objective here is to add all files intended for the current release.
      - script: |
          git diff --name-only --diff-filter=AMR HEAD^1 HEAD | xargs -I '{}' cp --parents -r '{}' $(Build.BinariesDirectory)
          mkdir -p $(Build.BinariesDirectory)/libraries/python/libs
          cp $(Build.Repository.LocalPath)/Databricks/libraries/python/dbxdemo/dist/*.* $(Build.BinariesDirectory)/libraries/python/libs
          mkdir -p $(Build.BinariesDirectory)/cicd-scripts
          cp $(Build.Repository.LocalPath)/Databricks/cicd-scripts/*.* $(Build.BinariesDirectory)/cicd-scripts
          mkdir -p $(Build.BinariesDirectory)/notebooks
          cp $(Build.Repository.LocalPath)/Databricks/notebooks/*.* $(Build.BinariesDirectory)/notebooks
        displayName: 'Get Changes'

      # Create the deployment artifact and then publish it to the
      # artifact repository.
      - task: ArchiveFiles@2
        inputs:
          rootFolderOrFile: '$(Build.BinariesDirectory)'
          includeRootFolder: false
          archiveType: 'zip'
          archiveFile: '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip'
          replaceExistingArchive: true

      - task: PublishBuildArtifacts@1
        inputs:
          ArtifactName: 'Databricks'

      # Copy the ARM templates and notebooks to a separate artifact to be used in building of environments

      - task: CopyFiles@2
        displayName: 'Include templates in the artifact'
        inputs:
          SourceFolder: '$(Build.Repository.LocalPath)/Databricks'
          Contents: '**'
          TargetFolder: '$(Build.ArtifactStagingDirectory)/ARM'

      - task: PublishBuildArtifacts@1
        displayName: 'Publish artifact'
        inputs:
          PathtoPublish: '$(Build.ArtifactStagingDirectory)/ARM'
          ArtifactName: 'databricksarm'
          publishLocation: 'Container'

- stage: DeployNotebook
  displayName: 'DeployNotebook'
  jobs:
  - job: DeployNotebook
    displayName: 'DeployNotebook'
    pool:
      vmImage: ubuntu-20.04
    steps:
      - task: DownloadBuildArtifacts@1
        inputs:
          buildType: 'current'
          downloadType: 'single'
          artifactName: 'Databricks'
          downloadPath: '$(System.DefaultWorkingDirectory)'

      - task: DownloadBuildArtifacts@1
        inputs:
          buildType: 'current'
          downloadType: 'single'
          artifactName: 'databricksarm'
          downloadPath: '$(System.DefaultWorkingDirectory)'

      - task: UsePythonVersion@0
        displayName: 'Use Python 3.8'
        inputs:
          versionSpec: 3.8


      - task: ExtractFiles@1
        displayName: 'Extract artifact files '
        inputs:
          destinationFolder: '$(System.ArtifactsDirectory)/Databricks'
          cleanDestinationFolder: false

      - task: Bash@3
        displayName: 'Install Databricks CLI and unittest XML reporting'
        inputs: 
            targetType: 'inline'
            script: |
              pip install databricks-cli
              pip install unittest-xml-reporting
              echo "[DEFAULT]" >> ~/.databrickscfg
              echo "host = $(DATABRICKS_ADDRESS)" >> ~/.databrickscfg
              echo "token = $(DATABRICKS_API_TOKEN)" >> ~/.databrickscfg
        
      - bash: 'databricks workspace import --language=PYTHON --format=SOURCE --overwrite "$(System.ArtifactsDirectory)/Databricks/notebooks/demonotebook.py" "/Shared/demonotebook.py"'
        displayName: 'Deploy notebook'

      